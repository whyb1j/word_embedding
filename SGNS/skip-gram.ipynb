{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import copy\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def read_data(data_path):\n",
    "    print(\"---读取文本数据---\")\n",
    "    english_stopwords = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    with open(data_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read().lower().split()\n",
    "    text = [word for word in text if word not in english_stopwords]\n",
    "    text = [stemmer.stem(word) for word in text]#>>10890638\n",
    "    np.save('sgns_data/text.npy',text)\n",
    "def map(data_path):\n",
    "    text=np.load(data_path)\n",
    "    vocab_dict = dict(Counter(text))# 得到单词字典表，key是单词，value是次数\n",
    "    word2idx = {word:i for i, word in enumerate(vocab_dict.keys())}\n",
    "    idx2word = {i:word for i, word in enumerate(vocab_dict.keys())}\n",
    "    word_counts = np.array([count for count in vocab_dict.values()], dtype=np.float32)\n",
    "    word_freqs =word_counts ** (3./4.)\n",
    "    word_freqs = word_freqs / np.sum(word_freqs)\n",
    "    return text,word2idx,idx2word,word_freqs\n",
    "class WordEmbeddingDataset(Dataset):\n",
    "    def __init__(self, text, word2idx, word_freqs,window_size,K):\n",
    "        ''' text: a list of words, all text from the training dataset\n",
    "            word2idx: the dictionary from word to index\n",
    "            word_freqs: the frequency of each word\n",
    "        '''\n",
    "        super().__init__() # #通过父类初始化模型，然后重写两个方法\n",
    "        self.text=text\n",
    "        self.text_encoded = [word2idx.get(word) for word in self.text] # 把文本转换成数字编码\n",
    "        self.text_encoded = torch.tensor(self.text_encoded,dtype=torch.int32)\n",
    "        self.word2idx = word2idx\n",
    "        self.word_freqs = torch.tensor(word_freqs,dtype=torch.float32)\n",
    "        self.window_size=window_size\n",
    "        self.K=K\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text_encoded) # 返回所有单词的总数，即item的总数\n",
    "    \n",
    "    def __getitem__(self, idx) -> tuple[torch.Tensor,torch.Tensor,torch.Tensor] :\n",
    "        ''' 这个function返回以下数据用于训练\n",
    "            - 中心词\n",
    "            - 这个单词附近的positive word\n",
    "            - 随机采样的K个单词作为negative word\n",
    "        '''\n",
    "        center_word = self.text_encoded[idx] # 取得中心词\n",
    "        left = list(range(idx - self.window_size, idx))\n",
    "        right = list(range(idx + 1, idx + self.window_size + 1))\n",
    "        pos_indices = [i % len(self.text) for i in left + right]  \n",
    "        # pos_indices=list(range(max(0,idx-self.window),min(idx+1+self.window,len(self.text))))\n",
    "        # pos_indices.remove(idx)      \n",
    "        pos_words = self.text_encoded[pos_indices]\n",
    "        select_weight = copy.deepcopy(self.word_freqs)\n",
    "        select_weight[pos_words] = 0\n",
    "        select_weight[center_word] = 0\n",
    "        neg_words = torch.multinomial(select_weight, self.K * pos_words.shape[0], True)\n",
    "        # torch.multinomial作用是对self.word_freqs做K * pos_words.shape[0]次取值，输出的是self.word_freqs对应的下标\n",
    "        # 取样方式采用有放回的采样，并且self.word_freqs数值越大，取样概率越大\n",
    "        # 每采样一个正确的单词(positive word)，就采样K个错误的单词(negative word)，pos_words.shape[0]是正确单词数量\n",
    "        # while 循环是为了保证 neg_words中不能包含背景词\n",
    "        return center_word, pos_words, neg_words\n",
    "class SGNS(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(SGNS, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size \n",
    "        self.in_embed = nn.Embedding(self.vocab_size, self.embed_size)\n",
    "        self.out_embed = nn.Embedding(self.vocab_size, self.embed_size)\n",
    "\n",
    "    def forward(self, center_word, target_word, negative_word):\n",
    "        ''' input_labels: center words, [batch_size, 1]\n",
    "            pos_labels: positive words, [batch_size, (window_size * 2)]\n",
    "            neg_labels:negative words, [batch_size, (window_size * 2 * K)]\n",
    "            \n",
    "            return: loss, [batch_size]  \n",
    "        '''\n",
    "        emb_v = self.in_embed(center_word) # [batch_size, 1, embed_size]\n",
    "        emb_u = self.out_embed(target_word) # [batch_size, (window * 2), embed_size]\n",
    "        emb_neg = self.out_embed(negative_word)  # [batch_size, (window * 2 * K), embed_size]      \n",
    "        emb_v=emb_v.unsqueeze(2)\n",
    "        pos_score =torch.bmm(emb_u, emb_v) #中心词和上下文词是一对一的因此逐个乘\n",
    "        pos_score =pos_score.squeeze(2)\n",
    "        neg_score = torch.bmm(emb_neg,emb_v)\n",
    "        #一个[中心词,上下文词]词对，对应K个负样本，因此要批量相乘\n",
    "        log_pos =  F.logsigmoid(pos_score).sum(1) # .sum()结果只为一个数，.sum(1)结果是一维的张量\n",
    "        log_neg =  F.logsigmoid(-1 * neg_score).sum(1)\n",
    "        loss = torch.sum(log_pos) + torch.sum(log_neg)\n",
    "        return -loss\n",
    "    def input_embedding(self):\n",
    "        return self.embedding_u.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text,word2idx,idx2word,word_freqs=map('text.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "window=2\n",
    "K=15\n",
    "running_loss=0\n",
    "vocab_size=len(word2idx)\n",
    "dataset=WordEmbeddingDataset(text,word2idx,word_freqs,window,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SGNS(vocab_size, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch_data in enumerate(dataloader):\n",
    "    if i == 1:\n",
    "        # 找到了我们想要的批次\n",
    "        input_data, positive_data, negative_data = batch_data\n",
    "        break  # 找到后退出循环\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_embed = nn.Embedding(vocab_size, 100)\n",
    "input_data, positive_data, negative_data = in_embed(input_data),in_embed(positive_data),in_embed(negative_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 100]) torch.Size([2, 4, 100]) torch.Size([2, 60, 100])\n"
     ]
    }
   ],
   "source": [
    "print(input_data.shape,positive_data.shape,negative_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5445319\n"
     ]
    }
   ],
   "source": [
    "print(len(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data=input_data.unsqueeze(2)\n",
    "x=torch.bmm(positive_data,input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 100, 1])\n"
     ]
    }
   ],
   "source": [
    "print(input_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz=torch.bmm(negative_data,input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-12.0917,   8.2146,   0.8796,   6.6761],\n",
      "        [  5.1587,   0.8796,  -4.4565,   1.4659]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "x=x.squeeze(2)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2092e+01, -2.7064e-04, -3.4708e-01, -1.2599e-03],\n",
      "        [-5.7325e-03, -3.4708e-01, -4.4680e+00, -2.0773e-01]],\n",
      "       grad_fn=<LogSigmoidBackward0>)\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2])\n",
      "tensor([-12.4403,  -5.0286], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "log_pos = F.logsigmoid(x)\n",
    "print(log_pos)\n",
    "print(log_pos.shape)\n",
    "log_pos=log_pos.sum(1)\n",
    "print(log_pos.shape)\n",
    "print(log_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-8.7344, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y=log_pos.mean()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-11.5785, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 100, 1])\n"
     ]
    }
   ],
   "source": [
    "print(input_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "batch1 must be a 3D tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpositive_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: batch1 must be a 3D tensor"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 100]) torch.Size([8, 4, 100]) torch.Size([8, 60, 100])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (4) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositive_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\.conda\\envs\\env2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\.conda\\envs\\env2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 90\u001b[0m, in \u001b[0;36mSGNS.forward\u001b[1;34m(self, center_word, target_word, negative_word)\u001b[0m\n\u001b[0;32m     88\u001b[0m emb_neg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_embed(negative_word)  \u001b[38;5;66;03m# [batch_size, (window * 2 * K), embed_size]      \u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(emb_v\u001b[38;5;241m.\u001b[39mshape,emb_u\u001b[38;5;241m.\u001b[39mshape,emb_neg\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 90\u001b[0m pos_score \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43memb_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb_u\u001b[49m\u001b[43m)\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m#中心词和上下文词是一对一的因此逐个乘\u001b[39;00m\n\u001b[0;32m     91\u001b[0m neg_score \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39mmul( emb_neg, emb_v),dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m#一个[中心词,上下文词]词对，对应K个负样本，因此要批量相乘\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (4) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "loss = model(input_data, positive_data, negative_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100]) torch.Size([1, 4, 100]) torch.Size([1, 60, 100])\n"
     ]
    }
   ],
   "source": [
    "input_labels, pos_labels, neg_labels =dataset[2]\n",
    "input_labels = input_labels.unsqueeze(0)  # 或者 input_labels.unsqueeze_(0)\n",
    "pos_labels = pos_labels.unsqueeze(0)    # 或者 pos_labels.unsqueeze_(0)\n",
    "neg_labels = neg_labels.unsqueeze(0)    \n",
    "loss = model(input_labels, pos_labels, neg_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252.2958984375\n"
     ]
    }
   ],
   "source": [
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "window=2\n",
    "K=15\n",
    "running_loss=0\n",
    "vocab_size=len(word2idx)\n",
    "dataset=WordEmbeddingDataset(text,word2idx,word_freqs,window,K)\n",
    "dataloader = DataLoader(dataset, batch_size=256,shuffle=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = SGNS(vocab_size, 100)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "for e in range(1):\n",
    "    print(\"---start---\")\n",
    "    for i, (input_labels, pos_labels, neg_labels) in enumerate(dataloader):\n",
    "        input_labels = input_labels.long().to(device)\n",
    "        pos_labels = pos_labels.long().to(device)\n",
    "        neg_labels = neg_labels.long().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(input_labels, pos_labels, neg_labels)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 20000 == 0:\n",
    "            print('epoch', e, 'iteration', i, running_loss/(i+1))\n",
    "torch.save(model.state_dict(), \"embedding.th\")\n",
    "embedding_weights = model.input_embedding()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdataset\u001b[49m[\u001b[38;5;241m2516\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, args.epoch + 1):\n",
    "        dataset = SGNSDataset(word2idx, text_idx, words_freq, args)\n",
    "        dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
    "        print('dataset size: {}'.format(len(dataset)))\n",
    "        print('batch num: {}'.format(len(dataloader)))\n",
    "        train_loss = 0\n",
    "        loss_o = 0\n",
    "        loss_n = 0\n",
    "        step = 0\n",
    "        avg_err = 0\n",
    "        print('Starting epoch: {}'.format(epoch))\n",
    "        for _, (iword, owords, nwords) in enumerate(dataloader):\n",
    "            step += 1\n",
    "            if epoch == 1 and step <= 1:\n",
    "                print('iword: {}, shape: {}, max: {}'.format(iword, iword.shape, iword.max()))\n",
    "                print('owords: {}, shape: {}, max: {}'.format(owords, owords.shape, owords.max()))\n",
    "                print('nwords: {}, shape: {}, max: {}'.format(nwords, nwords.shape, nwords.max()))\n",
    "            sys.stdout.flush()\n",
    "            optim.zero_grad()\n",
    "            if args.cuda == 'True':\n",
    "                iword, owords, nwords = iword.cuda(), owords.cuda(), nwords.cuda()\n",
    "            score_o, score_n = sgns(iword, owords, nwords)\n",
    "            # loss = torch.mean(score_o + score_n)\n",
    "            loss = score_o + score_n\n",
    "            train_loss += loss.item()\n",
    "            loss_o += score_o.item()\n",
    "            loss_n += score_n.item()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            # print the training stats\n",
    "            if step % 1000 == 0:\n",
    "                in_embed = sgns.get_embeddings('in')\n",
    "                avg_err = calc_err(in_embed, word2idx, args)\n",
    "                print('Epoch: {}, Step: {}, train_loss: {}, score_o: {}, score_n: {}, avg_error: {}'.format(epoch, step, loss.item(), score_o.item(), score_n.item(), avg_err))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "                # test the embedding\n",
    "                test_list = ['rain', 'utah', 'computer', 'brother', 'house']\n",
    "                for test_w in test_list:\n",
    "                    print('Words closest to chosen word: {}'.format(test_w))\n",
    "                    most_similar(test_w, in_embed, vocabulary, word2idx)\n",
    "\n",
    "        train_loss /= step\n",
    "        loss_o /= step\n",
    "        loss_n /= step\n",
    "        print('Finished Epoch: {}, train_loss: {}, loss_o: {}, loss_n: {}, avg error: {}'.format(epoch, train_loss, loss_o, loss_n, avg_err))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 100\n",
    "class SGNS(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(SGNS, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        self.in_embed = nn.Embedding(self.vocab_size, self.embed_size)\n",
    "        self.out_embed = nn.Embedding(self.vocab_size, self.embed_size)\n",
    "        \n",
    "        init_scale = 0.5 / self.embed_size\n",
    "        self.in_embed.weight.data.uniform_(-init_scale, init_scale)\n",
    "        self.out_embed.weight.data.uniform_(-init_scale, init_scale)\n",
    "    def forward(self, input_labels, pos_labels, neg_labels):\n",
    "        ''' input_labels: center words, [batch_size]\n",
    "            pos_labels: positive words, [batch_size, (window_size * 2)]\n",
    "            neg_labels:negative words, [batch_size, (window_size * 2 * K)]\n",
    "            \n",
    "            return: loss, [batch_size]\n",
    "        '''\n",
    "        input_embedding = self.in_embed(input_labels) # [batch_size, embed_size]\n",
    "        pos_embedding = self.out_embed(pos_labels)# [batch_size, (window * 2), embed_size]\n",
    "        neg_embedding = self.out_embed(neg_labels) # [batch_size, (window * 2 * K), embed_size]\n",
    "        \n",
    "        input_embedding = input_embedding.unsqueeze(2) # [batch_size, embed_size, 1]\n",
    "        \n",
    "        pos_dot = torch.bmm(pos_embedding, input_embedding) # [batch_size, (window * 2), 1]\n",
    "        pos_dot = pos_dot.squeeze(2) # [batch_size, (window * 2)]\n",
    "        \n",
    "        neg_dot = torch.bmm(neg_embedding, -input_embedding) # [batch_size, (window * 2 * K), 1]\n",
    "        neg_dot = neg_dot.squeeze(2) # batch_size, (window * 2 * K)]\n",
    "        \n",
    "        log_pos = F.logsigmoid(pos_dot).sum(1) # .sum()结果只为一个数，.sum(1)结果是一维的张量\n",
    "        log_neg = F.logsigmoid(neg_dot).sum(1)\n",
    "        \n",
    "        loss = log_pos + log_neg\n",
    "        \n",
    "        return -loss\n",
    "    \n",
    "    # def input_embedding(self):\n",
    "    #     return self.in_embed.weight.detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1])\n",
      "tensor([[[1]],\n",
      "\n",
      "        [[2]]])\n",
      "torch.Size([2, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([[1],[2]])\n",
    "print(x.shape)\n",
    "x=x.unsqueeze(1)\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "---start---\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=256,shuffle=True,num_workers=2,pin_memory=True,persistent_workers=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = SGNS(MAX_VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "for e in range(1):\n",
    "    print(\"---start---\")\n",
    "    for i, (input_labels, pos_labels, neg_labels) in enumerate(dataloader):\n",
    "        input_labels = input_labels.long().to(device)\n",
    "        pos_labels = pos_labels.long().to(device)\n",
    "        neg_labels = neg_labels.long().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(input_labels, pos_labels, neg_labels).mean()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('epoch', e, 'iteration', i, loss.item())\n",
    "# embedding_weights = model.input_embedding()\n",
    "torch.save(model.state_dict(), \"embedding-{}.th\".format(EMBEDDING_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "vocab_size=10000\n",
    "embed_size=100\n",
    "vec_sta=np.zeros((vocab_size,embed_size))\n",
    "model = SGNS(vocab_size, embed_size)\n",
    "# 加载模型的状态字典\n",
    "state_dict = torch.load('embedding-100.th')\n",
    "# 加载模型参数\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "for i in range(vocab_size):\n",
    "    word_vectors = model.in_embed(torch.tensor(i))\n",
    "    vec_sta[i]=word_vectors\n",
    "print(word_vectors.shape)\n",
    "# for i in range(vocab_size):\n",
    "#     word_vectors = model.in_embed(i)\n",
    "#     vec_sta[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_sta=model.in_embed.weight.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(vec_sta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m vocabulary\u001b[38;5;241m=\u001b[39m[word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mvocab_dict\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()]\n\u001b[0;32m      2\u001b[0m result\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordsim353_agreed.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab_dict' is not defined"
     ]
    }
   ],
   "source": [
    "vocabulary=[word for word in vocab_dict.keys()]\n",
    "result=[]\n",
    "with open('wordsim353_agreed.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split('\\t') \n",
    "        word1,word2=parts[1],parts[2]\n",
    "        if word1 in vocabulary :\n",
    "            index1 = word2idx[word1]\n",
    "        else :\n",
    "            index1 = 9999\n",
    "        if word2 in vocabulary:\n",
    "            index2 = word2idx[word2]  \n",
    "        else :\n",
    "            index2 = 9999\n",
    "        # 计算余弦相似度\n",
    "        vec1 = vec_sta[index1]\n",
    "        vec2 = vec_sta[index2]\n",
    "        cosine_sim = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "        result.append([word1,word2,cosine_sim])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tiger', 'cat', -0.12228242]\n"
     ]
    }
   ],
   "source": [
    "print(result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output_sgns.txt', 'w') as file:\n",
    "    # 遍历结果列表\n",
    "    for item in result:\n",
    "        # 将每个元素转换为字符串，并以制表符分隔\n",
    "        line = '\\t'.join(map(str, item))\n",
    "        # 写入文件\n",
    "        file.write(line + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbeddingDataset(Dataset):\n",
    "    def __init__(self, text, word2idx, word_freqs,window,K):\n",
    "        ''' text: a list of words, all text from the training dataset\n",
    "            word2idx: the dictionary from word to index\n",
    "            word_freqs: the frequency of each word\n",
    "        '''\n",
    "        super(WordEmbeddingDataset, self).__init__() # #通过父类初始化模型，然后重写两个方法\n",
    "        # self.text_encoded = [word2idx.get(word, word2idx['<UNK>']) for word in text] # 把文本数字化表示。如果不在词典中，也表示为unk\n",
    "        self.text_encoded = [word2idx.get(word,word2idx['<UNK>']) for word in text]\n",
    "        self.text_encoded = torch.LongTensor(self.text_encoded)\n",
    "        self.word2idx = word2idx\n",
    "        self.word_freqs = torch.Tensor(word_freqs)\n",
    "        self.window=window\n",
    "        self.K=K\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text_encoded) # 返回所有单词的总数，即item的总数\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ''' 这个function返回以下数据用于训练\n",
    "            - 中心词\n",
    "            - 这个单词附近的positive word\n",
    "            - 随机采样的K个单词作为negative word\n",
    "        '''\n",
    "        center_word = torch.LongTensor([self.text_encoded[idx]]) # 取得中心词\n",
    "        pos_indices = list(range(idx - self.window, idx)) + list(range(idx +1, idx + self.window + 1)) # 先取得中心左右各C个词的索引\n",
    "        pos_indices = [i % len(self.text_encoded) for i in pos_indices] # 为了避免索引越界，所以进行取余处理\n",
    "        pos_words = self.text_encoded[pos_indices]\n",
    "        # x=[-2,-1,0,1,2]\n",
    "        # pos_indices = [i % 10000 for i in x] <<[9998, 9999, 0, 1, 2]\n",
    "        select_weight = copy.deepcopy(self.word_freqs)\n",
    "        select_weight[pos_words] = 0\n",
    "        select_weight[center_word] = 0\n",
    "        neg_words = torch.multinomial(select_weight, self.K * pos_words.shape[0], True)\n",
    "        # torch.multinomial作用是对self.word_freqs做K * pos_words.shape[0]次取值，输出\n",
    "        # 的是self.word_freqs对应的下标\n",
    "        # 取样方式采用有放回的采样，并且self.word_freqs数值越大，取样概率越大\n",
    "        # 每采样一个正确的单词(positive word)，就采样K个错误的单词(negative word)，pos_words.shape[0]是正确单词数量\n",
    "        # while 循环是为了保证 neg_words中不能包含背景词\n",
    "        return center_word, pos_words, neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGNS(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(SGNS, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.embedding_v = nn.Embedding(self.vocab_size, self.embed_size, sparse=True)\n",
    "        self.embedding_u = nn.Embedding(self.vocab_size, self.embed_size, sparse=True)\n",
    "        \n",
    "    def forward(self, center_word, target_word, negative_word):\n",
    "        ''' input_labels: center words, [batch_size, 1]\n",
    "            pos_labels: positive words, [batch_size, (window_size * 2)]\n",
    "            neg_labels:negative words, [batch_size, (window_size * 2 * K)]\n",
    "            \n",
    "            return: loss, [batch_size]  \n",
    "        '''\n",
    "        print(center_word.shape,target_word.shape,negative_word.shape)\n",
    "        emb_v = self.embedding_v(center_word) # [batch_size, 1, embed_size]\n",
    "        emb_u = self.embedding_u(target_word) # [batch_size, (window * 2), embed_size]\n",
    "        emb_neg = self.embedding_u(negative_word)  # [batch_size, (window * 2 * K), embed_size]      \n",
    "        print(emb_v.shape,emb_u.shape,emb_neg.shape) \n",
    "        pos_score = torch.sum(torch.mul(emb_v, emb_u), dim=2) #中心词和上下文词是一对一的因此逐个乘\n",
    "        neg_score = torch.sum(torch.mul( emb_neg, emb_v),dim=2)\n",
    "        #一个[中心词,上下文词]词对，对应K个负样本，因此要批量相乘\n",
    "        log_pos =  F.logsigmoid(pos_score).squeeze() # .sum()结果只为一个数，.sum(1)结果是一维的张量\n",
    "        log_neg =  F.logsigmoid(-1 * neg_score).squeeze()\n",
    "        loss = log_pos + log_neg\n",
    "        return -loss\n",
    "    def input_embedding(self):\n",
    "        return self.embedding_u.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=WordEmbeddingDataset(text,word2idx,word_freqs,2,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([5233]), tensor([  15,   72, 3080,   11]), tensor([[ 433,   77, 5937,    7,   24, 7952, 5292,   14, 1530, 3741, 8741,  670,\n",
      "         1921,  112,    4],\n",
      "        [ 499, 2089,  786, 5164,   18, 6964, 2594, 9847, 4073,    3,   74, 5299,\n",
      "         2371,    2, 2485],\n",
      "        [   1,    0, 1354,  821, 4075,   24, 4855,    5,  175,   73, 4699,  936,\n",
      "          307, 9016, 7076],\n",
      "        [3636, 2822, 1257,  292,  157,  853,   76, 9999,  725,  573, 7469, 8666,\n",
      "         1303,  253, 3942]]))\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab():\n",
    "    with open('lmtraining.txt', 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    text=text.lower().split()\n",
    "    # print(type(text)) >>list\n",
    "    # print(len(text)) >>17005207\n",
    "    # vocab_dict = dict(Counter(text).most_common(MAX_VOCAB_SIZE - 1)) # 得到单词字典表，key是单词，value是次数\n",
    "    # vocab_dict['<UNK>'] = len(text) - np.sum(list(vocab_dict.values())) # 把不常用的单词都编码为\"<UNK>\"\n",
    "    vocab_dict=dict(Counter(text)) #total=253854\n",
    "    word2idx = {word:i for i, word in enumerate(vocab_dict.keys())}\n",
    "    idx2word = {i:word for i, word in enumerate(vocab_dict.keys())}\n",
    "    word_counts = np.array([count for count in vocab_dict.values()], dtype=np.float32)\n",
    "    word_freqs = word_counts ** (3./4.)\n",
    "    word_freqs = word_freqs / np.sum(word_freqs)\n",
    "    return text,vocab_dict,word2idx,idx2word,word_freqs\n",
    "MAX_VOCAB_SIZE=10000 #total=253854\n",
    "text,vocab_dict,word2idx,idx2word,word_freqs=build_vocab()\n",
    "vocab_size=len(vocab_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "vocab_size=len(vocab_dict)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataloader\u001b[38;5;241m=\u001b[39mDataLoader(\u001b[43mdataset\u001b[49m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m SGNS(vocab_size,MAX_VOCAB_SIZE )\n\u001b[0;32m      3\u001b[0m input_labels, pos_labels, neg_labels\u001b[38;5;241m=\u001b[39mdataset[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataloader=DataLoader(dataset,1)\n",
    "model = SGNS(vocab_size,MAX_VOCAB_SIZE )\n",
    "input_labels, pos_labels, neg_labels=dataset[0]\n",
    "loss = model(input_labels, pos_labels, neg_labels)\n",
    "print(loss.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
