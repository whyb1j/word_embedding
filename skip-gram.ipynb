{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "MAX_VOCAB_SIZE=10000 #total=253854\n",
    "with open('lmtraining.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "text=text.lower().split()\n",
    "# print(type(text)) >>list\n",
    "# print(len(text)) >>17005207\n",
    "vocab_dict = dict(Counter(text).most_common(MAX_VOCAB_SIZE - 1)) # 得到单词字典表，key是单词，value是次数\n",
    "vocab_dict['<UNK>'] = len(text) - np.sum(list(vocab_dict.values())) # 把不常用的单词都编码为\"<UNK>\"\n",
    "word2idx = {word:i for i, word in enumerate(vocab_dict.keys())}\n",
    "idx2word = {i:word for i, word in enumerate(vocab_dict.keys())}\n",
    "word_counts = np.array([count for count in vocab_dict.values()], dtype=np.float32)\n",
    "word_freqs =word_counts ** (3./4.)\n",
    "word_freqs = word_freqs / np.sum(word_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "z=list(vocab_dict.values())\n",
    "print(z[19998])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126\n"
     ]
    }
   ],
   "source": [
    "min=10000\n",
    "for i in vocab_dict.values():\n",
    "    if i<min:\n",
    "        min=i\n",
    "print(min) #126\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "class WordEmbeddingDataset(Dataset):\n",
    "    def __init__(self, text, word2idx, word_freqs,window,K):\n",
    "        ''' text: a list of words, all text from the training dataset\n",
    "            word2idx: the dictionary from word to index\n",
    "            word_freqs: the frequency of each word\n",
    "        '''\n",
    "        super(WordEmbeddingDataset, self).__init__() # #通过父类初始化模型，然后重写两个方法\n",
    "        self.text_encoded = [word2idx.get(word, word2idx['<UNK>']) for word in text] # 把文本数字化表示。如果不在词典中，也表示为unk\n",
    "        self.text_encoded = torch.LongTensor(self.text_encoded)\n",
    "        self.word2idx = word2idx\n",
    "        self.word_freqs = torch.Tensor(word_freqs)\n",
    "        self.window=window\n",
    "        self.K=K\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text_encoded) # 返回所有单词的总数，即item的总数\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ''' 这个function返回以下数据用于训练\n",
    "            - 中心词\n",
    "            - 这个单词附近的positive word\n",
    "            - 随机采样的K个单词作为negative word\n",
    "        '''\n",
    "        center_word = self.text_encoded[idx] # 取得中心词\n",
    "        pos_indices = list(range(idx - self.window, idx)) + list(range(idx +1, idx + self.window + 1)) # 先取得中心左右各C个词的索引\n",
    "        pos_indices = [i % len(self.text_encoded) for i in pos_indices] # 为了避免索引越界，所以进行取余处理\n",
    "        pos_words = self.text_encoded[pos_indices]\n",
    "        # x=[-2,-1,0,1,2]\n",
    "        # pos_indices = [i % 10000 for i in x] <<[9998, 9999, 0, 1, 2]\n",
    "        select_weight = copy.deepcopy(self.word_freqs)\n",
    "        select_weight[pos_words] = 0\n",
    "        select_weight[center_word] = 0\n",
    "        neg_words = torch.multinomial(select_weight, self.K * pos_words.shape[0], True)\n",
    "        # torch.multinomial作用是对self.word_freqs做K * pos_words.shape[0]次取值，输出的是self.word_freqs对应的下标\n",
    "        # 取样方式采用有放回的采样，并且self.word_freqs数值越大，取样概率越大\n",
    "        # 每采样一个正确的单词(positive word)，就采样K个错误的单词(negative word)，pos_words.shape[0]是正确单词数量\n",
    "        # while 循环是为了保证 neg_words中不能包含背景词\n",
    "        return center_word, pos_words, neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n"
     ]
    }
   ],
   "source": [
    "print([x for x in range(1,3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WordEmbeddingDataset(text, word2idx, word_freqs,2,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5233),\n",
       " tensor([  15,   72, 3080,   11]),\n",
       " tensor([   0, 1487,    1, 1761, 1904, 2113, 1431, 8095, 9999,  420,  437,   59,\n",
       "          703,   89, 1173, 2294, 1207, 6486, 6267,  138,   35,    2, 3453, 9999,\n",
       "         1049, 9999, 3800,    5, 4378,  242,  974, 1529,   97, 2369, 9999,  378,\n",
       "          719,    2,   27, 3877,  163, 9999, 5427, 7108, 9999, 2944, 2191, 2173,\n",
       "          113,   65,  710,   24,   16, 6522, 1599,    1,  425, 3900, 9999,  158]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 100\n",
    "class SGNS(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(SGNS, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        self.in_embed = nn.Embedding(self.vocab_size, self.embed_size)\n",
    "        self.out_embed = nn.Embedding(self.vocab_size, self.embed_size)\n",
    "        \n",
    "    def forward(self, input_labels, pos_labels, neg_labels):\n",
    "        ''' input_labels: center words, [batch_size]\n",
    "            pos_labels: positive words, [batch_size, (window_size * 2)]\n",
    "            neg_labels:negative words, [batch_size, (window_size * 2 * K)]\n",
    "            \n",
    "            return: loss, [batch_size]\n",
    "        '''\n",
    "        input_embedding = self.in_embed(input_labels) # [batch_size, embed_size]\n",
    "        pos_embedding = self.out_embed(pos_labels)# [batch_size, (window * 2), embed_size]\n",
    "        neg_embedding = self.out_embed(neg_labels) # [batch_size, (window * 2 * K), embed_size]\n",
    "        \n",
    "        input_embedding = input_embedding.unsqueeze(2) # [batch_size, embed_size, 1]\n",
    "        \n",
    "        pos_dot = torch.bmm(pos_embedding, input_embedding) # [batch_size, (window * 2), 1]\n",
    "        pos_dot = pos_dot.squeeze(2) # [batch_size, (window * 2)]\n",
    "        \n",
    "        neg_dot = torch.bmm(neg_embedding, -input_embedding) # [batch_size, (window * 2 * K), 1]\n",
    "        neg_dot = neg_dot.squeeze(2) # batch_size, (window * 2 * K)]\n",
    "        \n",
    "        log_pos = F.logsigmoid(pos_dot).sum(1) # .sum()结果只为一个数，.sum(1)结果是一维的张量\n",
    "        log_neg = F.logsigmoid(neg_dot).sum(1)\n",
    "        \n",
    "        loss = log_pos + log_neg\n",
    "        \n",
    "        return -loss\n",
    "    \n",
    "    # def input_embedding(self):\n",
    "    #     return self.in_embed.weight.detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1])\n",
      "tensor([[[1]],\n",
      "\n",
      "        [[2]]])\n",
      "torch.Size([2, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([[1],[2]])\n",
    "print(x.shape)\n",
    "x=x.unsqueeze(1)\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "---start---\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=256,shuffle=True,num_workers=2,pin_memory=True,persistent_workers=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = SGNS(MAX_VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "for e in range(1):\n",
    "    print(\"---start---\")\n",
    "    for i, (input_labels, pos_labels, neg_labels) in enumerate(dataloader):\n",
    "        input_labels = input_labels.long().to(device)\n",
    "        pos_labels = pos_labels.long().to(device)\n",
    "        neg_labels = neg_labels.long().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(input_labels, pos_labels, neg_labels).mean()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('epoch', e, 'iteration', i, loss.item())\n",
    "# embedding_weights = model.input_embedding()\n",
    "torch.save(model.state_dict(), \"embedding-{}.th\".format(EMBEDDING_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "vocab_size=10000\n",
    "embed_size=100\n",
    "vec_sta=np.zeros((vocab_size,embed_size))\n",
    "model = SGNS(vocab_size, embed_size)\n",
    "# 加载模型的状态字典\n",
    "state_dict = torch.load('embedding-100.th')\n",
    "# 加载模型参数\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "for i in range(vocab_size):\n",
    "    word_vectors = model.in_embed(torch.tensor(i))\n",
    "    vec_sta[i]=word_vectors\n",
    "print(word_vectors.shape)\n",
    "# for i in range(vocab_size):\n",
    "#     word_vectors = model.in_embed(i)\n",
    "#     vec_sta[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_sta=model.in_embed.weight.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(vec_sta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m vocabulary\u001b[38;5;241m=\u001b[39m[word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mvocab_dict\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()]\n\u001b[0;32m      2\u001b[0m result\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordsim353_agreed.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab_dict' is not defined"
     ]
    }
   ],
   "source": [
    "vocabulary=[word for word in vocab_dict.keys()]\n",
    "result=[]\n",
    "with open('wordsim353_agreed.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split('\\t') \n",
    "        word1,word2=parts[1],parts[2]\n",
    "        if word1 in vocabulary :\n",
    "            index1 = word2idx[word1]\n",
    "        else :\n",
    "            index1 = 9999\n",
    "        if word2 in vocabulary:\n",
    "            index2 = word2idx[word2]  \n",
    "        else :\n",
    "            index2 = 9999\n",
    "        # 计算余弦相似度\n",
    "        vec1 = vec_sta[index1]\n",
    "        vec2 = vec_sta[index2]\n",
    "        cosine_sim = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "        result.append([word1,word2,cosine_sim])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tiger', 'cat', -0.12228242]\n"
     ]
    }
   ],
   "source": [
    "print(result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output_sgns.txt', 'w') as file:\n",
    "    # 遍历结果列表\n",
    "    for item in result:\n",
    "        # 将每个元素转换为字符串，并以制表符分隔\n",
    "        line = '\\t'.join(map(str, item))\n",
    "        # 写入文件\n",
    "        file.write(line + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbeddingDataset(Dataset):\n",
    "    def __init__(self, text, word2idx, word_freqs,window,K):\n",
    "        ''' text: a list of words, all text from the training dataset\n",
    "            word2idx: the dictionary from word to index\n",
    "            word_freqs: the frequency of each word\n",
    "        '''\n",
    "        super(WordEmbeddingDataset, self).__init__() # #通过父类初始化模型，然后重写两个方法\n",
    "        # self.text_encoded = [word2idx.get(word, word2idx['<UNK>']) for word in text] # 把文本数字化表示。如果不在词典中，也表示为unk\n",
    "        self.text_encoded = [word2idx.get(word,word2idx['<UNK>']) for word in text]\n",
    "        self.text_encoded = torch.LongTensor(self.text_encoded)\n",
    "        self.word2idx = word2idx\n",
    "        self.word_freqs = torch.Tensor(word_freqs)\n",
    "        self.window=window\n",
    "        self.K=K\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text_encoded) # 返回所有单词的总数，即item的总数\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ''' 这个function返回以下数据用于训练\n",
    "            - 中心词\n",
    "            - 这个单词附近的positive word\n",
    "            - 随机采样的K个单词作为negative word\n",
    "        '''\n",
    "        center_word = torch.LongTensor([self.text_encoded[idx]]) # 取得中心词\n",
    "        pos_indices = list(range(idx - self.window, idx)) + list(range(idx +1, idx + self.window + 1)) # 先取得中心左右各C个词的索引\n",
    "        pos_indices = [i % len(self.text_encoded) for i in pos_indices] # 为了避免索引越界，所以进行取余处理\n",
    "        pos_words = self.text_encoded[pos_indices]\n",
    "        # x=[-2,-1,0,1,2]\n",
    "        # pos_indices = [i % 10000 for i in x] <<[9998, 9999, 0, 1, 2]\n",
    "        select_weight = copy.deepcopy(self.word_freqs)\n",
    "        select_weight[pos_words] = 0\n",
    "        select_weight[center_word] = 0\n",
    "        neg_words = torch.multinomial(select_weight, self.K * pos_words.shape[0], True)\n",
    "        # torch.multinomial作用是对self.word_freqs做K * pos_words.shape[0]次取值，输出\n",
    "        # 的是self.word_freqs对应的下标\n",
    "        # 取样方式采用有放回的采样，并且self.word_freqs数值越大，取样概率越大\n",
    "        # 每采样一个正确的单词(positive word)，就采样K个错误的单词(negative word)，pos_words.shape[0]是正确单词数量\n",
    "        # while 循环是为了保证 neg_words中不能包含背景词\n",
    "        return center_word, pos_words, neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGNS(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(SGNS, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.embedding_v = nn.Embedding(self.vocab_size, self.embed_size, sparse=True)\n",
    "        self.embedding_u = nn.Embedding(self.vocab_size, self.embed_size, sparse=True)\n",
    "        \n",
    "    def forward(self, center_word, target_word, negative_word):\n",
    "        ''' input_labels: center words, [batch_size, 1]\n",
    "            pos_labels: positive words, [batch_size, (window_size * 2)]\n",
    "            neg_labels:negative words, [batch_size, (window_size * 2 * K)]\n",
    "            \n",
    "            return: loss, [batch_size]  \n",
    "        '''\n",
    "        print(center_word.shape,target_word.shape,negative_word.shape)\n",
    "        emb_v = self.embedding_v(center_word) # [batch_size, 1, embed_size]\n",
    "        emb_u = self.embedding_u(target_word) # [batch_size, (window * 2), embed_size]\n",
    "        emb_neg = self.embedding_u(negative_word)  # [batch_size, (window * 2 * K), embed_size]      \n",
    "        print(emb_v.shape,emb_u.shape,emb_neg.shape) \n",
    "        pos_score = torch.sum(torch.mul(emb_v, emb_u), dim=2) #中心词和上下文词是一对一的因此逐个乘\n",
    "        neg_score = torch.sum(torch.mul( emb_neg, emb_v),dim=2)\n",
    "        #一个[中心词,上下文词]词对，对应K个负样本，因此要批量相乘\n",
    "        log_pos =  F.logsigmoid(pos_score).squeeze() # .sum()结果只为一个数，.sum(1)结果是一维的张量\n",
    "        log_neg =  F.logsigmoid(-1 * neg_score).squeeze()\n",
    "        loss = log_pos + log_neg\n",
    "        return -loss\n",
    "    def input_embedding(self):\n",
    "        return self.embedding_u.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=WordEmbeddingDataset(text,word2idx,word_freqs,2,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([5233]), tensor([  15,   72, 3080,   11]), tensor([[ 433,   77, 5937,    7,   24, 7952, 5292,   14, 1530, 3741, 8741,  670,\n",
      "         1921,  112,    4],\n",
      "        [ 499, 2089,  786, 5164,   18, 6964, 2594, 9847, 4073,    3,   74, 5299,\n",
      "         2371,    2, 2485],\n",
      "        [   1,    0, 1354,  821, 4075,   24, 4855,    5,  175,   73, 4699,  936,\n",
      "          307, 9016, 7076],\n",
      "        [3636, 2822, 1257,  292,  157,  853,   76, 9999,  725,  573, 7469, 8666,\n",
      "         1303,  253, 3942]]))\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab():\n",
    "    with open('lmtraining.txt', 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    text=text.lower().split()\n",
    "    # print(type(text)) >>list\n",
    "    # print(len(text)) >>17005207\n",
    "    # vocab_dict = dict(Counter(text).most_common(MAX_VOCAB_SIZE - 1)) # 得到单词字典表，key是单词，value是次数\n",
    "    # vocab_dict['<UNK>'] = len(text) - np.sum(list(vocab_dict.values())) # 把不常用的单词都编码为\"<UNK>\"\n",
    "    vocab_dict=dict(Counter(text)) #total=253854\n",
    "    word2idx = {word:i for i, word in enumerate(vocab_dict.keys())}\n",
    "    idx2word = {i:word for i, word in enumerate(vocab_dict.keys())}\n",
    "    word_counts = np.array([count for count in vocab_dict.values()], dtype=np.float32)\n",
    "    word_freqs = word_counts ** (3./4.)\n",
    "    word_freqs = word_freqs / np.sum(word_freqs)\n",
    "    return text,vocab_dict,word2idx,idx2word,word_freqs\n",
    "MAX_VOCAB_SIZE=10000 #total=253854\n",
    "text,vocab_dict,word2idx,idx2word,word_freqs=build_vocab()\n",
    "vocab_size=len(vocab_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "vocab_size=len(vocab_dict)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataloader\u001b[38;5;241m=\u001b[39mDataLoader(\u001b[43mdataset\u001b[49m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m SGNS(vocab_size,MAX_VOCAB_SIZE )\n\u001b[0;32m      3\u001b[0m input_labels, pos_labels, neg_labels\u001b[38;5;241m=\u001b[39mdataset[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataloader=DataLoader(dataset,1)\n",
    "model = SGNS(vocab_size,MAX_VOCAB_SIZE )\n",
    "input_labels, pos_labels, neg_labels=dataset[0]\n",
    "loss = model(input_labels, pos_labels, neg_labels)\n",
    "print(loss.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
